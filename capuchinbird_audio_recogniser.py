# -*- coding: utf-8 -*-
"""Vinayak_Audio Classification EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HMrTEFClNV5el_dzmAo9l-XFRmjkw8cH
"""

##### Audio Classification Using Machine Learning

!pip install librosa

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import drive

drive.mount('/content/gdrive/', force_remount=True)

!unzip "/content/gdrive/MyDrive/Colab Notebooks/archive.zip"

filename='/content/Parsed_Not_Capuchinbird_Clips/Crickets-chirping-0.wav'

import IPython.display as ipd
import librosa
import librosa.display

!dir
import os
os.getcwd()

### cricket Sound
plt.figure(figsize=(14,5))
data,sample_rate=librosa.load(filename)
librosa.display.waveplot(data,sr=sample_rate)
ipd.Audio(filename)

### Sound
filename='/content/Parsed_Not_Capuchinbird_Clips/Crickets-chirping-0.wav'
plt.figure(figsize=(14,5))
data,sample_rate=librosa.load(filename)
librosa.display.waveplot(data,sr=sample_rate)
ipd.Audio(filename)

sample_rate

from scipy.io import wavfile as wav
wave_sample_rate, wave_audio=wav.read(filename)

wave_sample_rate

wave_audio

data



"""#### Audio Classification Data Preprocessing"""

### Let's read a sample audio using librosa
import librosa
audio_file_path='/content/Parsed_Not_Capuchinbird_Clips/Crickets-chirping-0.wav'

librosa_audio_data,librosa_sample_rate=librosa.load(audio_file_path)

print(librosa_audio_data)

### Lets plot the librosa audio data
import matplotlib.pyplot as plt
# Original audio with 1 channel 
plt.figure(figsize=(12, 4))
plt.plot(librosa_audio_data)

"""#### Observation
Here Librosa converts the signal to mono, meaning the channel will alays be 1
"""

### Lets read with scipy
from scipy.io import wavfile as wav
wave_sample_rate, wave_audio = wav.read(audio_file_path)

wave_audio

import matplotlib.pyplot as plt

# Original audio with 2 channels 
plt.figure(figsize=(12, 4))
plt.plot(wave_audio)

"""### Extract Features
Here we will be using Mel-Frequency Cepstral Coefficients(MFCC) from the audio 
samples.
The MFCC summarises the frequency distribution across the window size, so it is possible to analyse both the frequency and time characteristics of the sound. These audio representations will allow us to identify features for classification.
"""

mfccs = librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_sample_rate, n_mfcc=40)
print(mfccs.shape)

mfccs

def features_extractor(file_name):
    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') 
    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)
    
    return mfccs_scaled_features

#### Extracting MFCC's For every audio file
import pandas as pd
import os
import librosa
base = '/content/'
class_label = ['Parsed_Not_Capuchinbird_Clips',"Parsed_Capuchinbird_Clips"]

base_path = base + class_label[0]
audio_file_path='/content/Parsed_Not_Capuchinbird_Clips/Crickets-chirping-0.wav'

from glob import glob
import numpy as np

base = '/content/'
class_label = ['Parsed_Not_Capuchinbird_Clips',"Parsed_Capuchinbird_Clips"]
extracted_features =[]
for class_val in class_label:
  base_path = base + class_val + "/"
  print(base_path )
  filename_path = glob(base_path+'*.wav')
  # print(filename_path)
  for filename in filename_path:
    print(filename)
    data=features_extractor(filename)
    extracted_features.append([data,class_val])

### converting extracted_features to Pandas dataframe
extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])
extracted_features_df.tail()

### Split the dataset into independent and dependent dataset
X=np.array(extracted_features_df['feature'].tolist())
y=np.array(extracted_features_df['class'].tolist())

### Label Encoding
###y=np.array(pd.get_dummies(y))
### Label Encoder
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
labelencoder=LabelEncoder()
y=to_categorical(labelencoder.fit_transform(y))

#binary labling
# y2=[]
# for i in y:
#   # print(i)
#   if i == 'Parsed_Not_Capuchinbird_Clips':
#     y2.append([0., 1.])
#   else:
#     y2.append([1., 0.])

### Train Test Split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)

X_train.shape

X_test.shape

y_test.shape

"""### Model Creation"""

import tensorflow as tf
print(tf.__version__)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten
from tensorflow.keras.optimizers import Adam
from sklearn import metrics

model=Sequential()
###first layer
model.add(Dense(100,input_shape=(40,)))
model.add(Activation('relu'))
model.add(Dropout(0.5))
###second layer
model.add(Dense(200))
model.add(Activation('relu'))
model.add(Dropout(0.5))
###third layer
model.add(Dense(100))
model.add(Activation('relu'))
model.add(Dropout(0.5))

###final layer
model.add(Dense(num_labels))
model.add(Activation('softmax'))

model.summary()

model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')

## Trianing my model
from tensorflow.keras.callbacks import ModelCheckpoint
from datetime import datetime 

num_epochs = 100
num_batch_size = 32

checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', 
                               verbose=1, save_best_only=True)
start = datetime.now()

model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)


duration = datetime.now() - start
print("Training completed in time: ", duration)

test_accuracy=model.evaluate(X_test,y_test,verbose=0)
print(test_accuracy[1])

X_test[1]

model.predict(X_test)

"""### Testing Some Test Audio Data

Steps
- Preprocess the new audio data
- predict the classes
- Invere transform your Predicted Label
"""

# give any path from Drive to test the model

filename = '/content/gdrive/MyDrive/Colab Notebooks/test_files/capichinbird1.wav'


audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') 
mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)

mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)
predicted_label = model.predict(mfccs_scaled_features)

# predicted_label=model.predict_classes(mfccs_scaled_features)
print(predicted_label[0])

predict_index=list(predicted_label[0]).index(max(predicted_label[0]))
predicted_lable_value=[]
for i in range(len(predicted_label[0])-1):
  predicted_lable_value.append(False)
predicted_lable_value.insert(predict_index,True)

# I created based multiple classes currently only binary

print(predicted_lable_value)
prediction_class = labelencoder.inverse_transform(predicted_lable_value)
print(prediction_class)